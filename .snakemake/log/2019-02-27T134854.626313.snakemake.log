Building DAG of jobs...
Using shell: /anaconda2/envs/py3/bin/bash
Provided cores: 1
Rules claiming more threads will be scaled down.
Job counts:
	count	jobs
	1	download_all
	1	parse
	2

[Wed Feb 27 13:48:56 2019]
Job 8: Parsing fasta into sequences and metadata

[Wed Feb 27 13:48:56 2019]
Error in rule parse:
    jobid: 8
    output: results/sequences_h3n2_ha.fasta, results/metadata_h3n2_ha.tsv

RuleException:
CalledProcessError in line 209 of /Users/katekistler/nextstrain/egg-passage/Snakefile:
Command 'set -euo pipefail;  augur parse             --sequences data/h3n2_ha.fasta             --output-sequences results/sequences_h3n2_ha.fasta             --output-metadata results/metadata_h3n2_ha.tsv             --fields strain virus accession date region country division location passage authors age gender' returned non-zero exit status 127.
  File "/Users/katekistler/nextstrain/egg-passage/Snakefile", line 209, in __rule_parse
  File "/anaconda2/envs/py3/lib/python3.6/concurrent/futures/thread.py", line 56, in run
Shutting down, this might take some time.
Exiting because a job execution failed. Look above for error message
Complete log: /Users/katekistler/nextstrain/egg-passage/.snakemake/log/2019-02-27T134854.626313.snakemake.log
